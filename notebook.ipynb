{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 309,
   "id": "6d03a9f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os, json, cv2, pickle, xmltodict, shutil, yaml, random\n",
    "from pathlib import Path\n",
    "import numpy as np\n",
    "from chitra.image import Chitra\n",
    "from matplotlib import pyplot as plt\n",
    "from PIL import Image\n",
    "from tqdm import tqdm\n",
    "from copy import deepcopy\n",
    "import pandas as pd\n",
    "\n",
    "ROOT = \"root_3\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cc662853",
   "metadata": {},
   "source": [
    "# UAV wildfire dataset\n",
    "\n",
    "## Для составления датасета были использованы следующие источники:\n",
    "\n",
    "1. [Aerial Imagery dataset for fire detection: classification and segmentation using Unmanned Aerial Vehicle (UAV)](https://github.com/AlirezaShamsoshoara/Fire-Detection-UAV-Aerial-Image-Classification-Segmentation-UnmannedAerialVehicle)\n",
    "2. [UAV Fire Detection](https://github.com/andre3racks/UAV-fire-detection)\n",
    "3. [Forest Fire Detection through UAV imagery using CNNs](https://github.com/LeadingIndiaAI/Forest-Fire-Detection-through-UAV-imagery-using-CNNs)\n",
    "4. [UAV Thermal Imaginary - Fire Dataset](https://www.kaggle.com/datasets/adiyeceran/uav-thermal-imaginary-fire-dataset)\n",
    "\n",
    "Различные датасеты были предназначены для различных задач: классификации, локализации, сегментации.<br>\n",
    "В датасете содержатся снимки полученные как с оптических, так и с тепловизионных камер.<br>\n",
    "В этом ноутбуке показано каким образом собирался общий датасет.<br>\n",
    "<p>Cтруктура датасета:</p>\n",
    "\n",
    "`./uav_wildfire_dataset/`+<br>\n",
    "&ensp;&emsp;&emsp;&emsp;&emsp;&emsp;&emsp;&emsp;&emsp;&emsp;&emsp;&emsp;&emsp;&emsp;|---`images/`+<br>\n",
    "&ensp;&emsp;&emsp;&emsp;&emsp;&emsp;&emsp;&emsp;&emsp;&emsp;&emsp;&emsp;&emsp;&emsp;|&nbsp;&ensp;&emsp;&emsp;&emsp;&emsp;&emsp;|---`<hex_id>.jpg`<br>\n",
    "&ensp;&emsp;&emsp;&emsp;&emsp;&emsp;&emsp;&emsp;&emsp;&emsp;&emsp;&emsp;&emsp;&emsp;|&nbsp;&ensp;&emsp;&emsp;&emsp;&emsp;&emsp;|---`...`<br>\n",
    "&ensp;&emsp;&emsp;&emsp;&emsp;&emsp;&emsp;&emsp;&emsp;&emsp;&emsp;&emsp;&emsp;&emsp;|<br>\n",
    "&ensp;&emsp;&emsp;&emsp;&emsp;&emsp;&emsp;&emsp;&emsp;&emsp;&emsp;&emsp;&emsp;&emsp;|<br>\n",
    "&ensp;&emsp;&emsp;&emsp;&emsp;&emsp;&emsp;&emsp;&emsp;&emsp;&emsp;&emsp;&emsp;&emsp;|---`masks/`+<br>\n",
    "&ensp;&ensp;&ensp;&emsp;&emsp;&emsp;&emsp;&emsp;&emsp;&emsp;&emsp;&emsp;&emsp;&emsp;&emsp;|&ensp;&ensp;&nbsp;&emsp;&emsp;&emsp;&emsp;|---`<hex_id>.jpg`<br>\n",
    "&ensp;&ensp;&ensp;&emsp;&emsp;&emsp;&emsp;&emsp;&emsp;&emsp;&emsp;&emsp;&emsp;&emsp;&emsp;|&ensp;&ensp;&nbsp;&emsp;&emsp;&emsp;&emsp;|---`...`<br>\n",
    "&ensp;&emsp;&emsp;&emsp;&emsp;&emsp;&emsp;&emsp;&emsp;&emsp;&emsp;&emsp;&emsp;&emsp;|<br>\n",
    "&ensp;&emsp;&emsp;&emsp;&emsp;&emsp;&emsp;&emsp;&emsp;&emsp;&emsp;&emsp;&emsp;&emsp;|<br>\n",
    "&ensp;&emsp;&emsp;&emsp;&emsp;&emsp;&emsp;&emsp;&emsp;&emsp;&emsp;&emsp;&emsp;&emsp;|---`bboxes/`+<br>\n",
    "&ensp;&ensp;&ensp;&emsp;&emsp;&emsp;&emsp;&emsp;&emsp;&emsp;&emsp;&emsp;&emsp;&emsp;&emsp;|&nbsp;&ensp;&emsp;&emsp;&emsp;&emsp;&emsp;|---`<hex_id>.csv`<br>\n",
    "&ensp;&ensp;&ensp;&emsp;&emsp;&emsp;&emsp;&emsp;&emsp;&emsp;&emsp;&emsp;&emsp;&emsp;&emsp;|&nbsp;&ensp;&emsp;&emsp;&emsp;&emsp;&emsp;|---`...`<br>\n",
    "&ensp;&emsp;&emsp;&emsp;&emsp;&emsp;&emsp;&emsp;&emsp;&emsp;&emsp;&emsp;&emsp;&emsp;|<br>\n",
    "&ensp;&emsp;&emsp;&emsp;&emsp;&emsp;&emsp;&emsp;&emsp;&emsp;&emsp;&emsp;&emsp;&emsp;|<br>\n",
    "&ensp;&emsp;&emsp;&emsp;&emsp;&emsp;&emsp;&emsp;&emsp;&emsp;&emsp;&emsp;&emsp;&emsp;|---`labels.csv`<br>\n",
    "&ensp;&emsp;&emsp;&emsp;&emsp;&emsp;&emsp;&emsp;&emsp;&emsp;&emsp;&emsp;&emsp;&emsp;|<br>\n",
    "&ensp;&emsp;&emsp;&emsp;&emsp;&emsp;&emsp;&emsp;&emsp;&emsp;&emsp;&emsp;&emsp;&emsp;|<br>\n",
    "&ensp;&emsp;&emsp;&emsp;&emsp;&emsp;&emsp;&emsp;&emsp;&emsp;&emsp;&emsp;&emsp;&emsp;+---`idx.json`<br>\n",
    "\n",
    "В папке `images` содержатся все изображения размера 640 x 640 со своими hex_id.<br>\n",
    "В папке `masks` содержатся маски, соответствующие hex_id изображения.<br>\n",
    "В папке `bboxes` содержатся координаты всех прямоугольников. ID описания соответствует hex_id изображения.<br>\n",
    "Файл `labels.csv` содержит две колонки: hex_id и label (1 - есть огонь, 0 - нет огня)<br>\n",
    "Файл `idx.json` содержит данные, необходимые для построения датасета под нужную задачу."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 310,
   "id": "e79160ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ImageDataset:\n",
    "    def __init__(self, root_posix_path: str, ext: tuple=(640, 640), restart=True, idx_b=None):\n",
    "        self.root_path = Path(root_posix_path)\n",
    "        self.root_path.mkdir(exist_ok=True, parents=True)\n",
    "        for sub_path in [\"images\", \"masks\", \"bboxes\"]:\n",
    "            path_ = self.root_path / sub_path\n",
    "            path_.mkdir(exist_ok=True, parents=True)\n",
    "        self.idx_keys = \"dataset_name\", \"label\", \"bboxes\", \"mask\", \"thermal\"\n",
    "        self.dataset_name, self.label, self.bboxes, self.mask, self.thermal = [None] * 5\n",
    "        self.ext = 640, 640\n",
    "        self.idx = {}\n",
    "        if idx_b:\n",
    "            self.idx = idx_b\n",
    "            self.current_id = int(list(idx_b.keys())[-1], 16) + 1\n",
    "            self.hex_id = self.generate_next_id()\n",
    "        if restart:\n",
    "            self.current_id = 0\n",
    "            self.hex_id = \"0\"\n",
    "            for file in [\"labels.csv\", \"idx.json\"]:\n",
    "                file_path = self.root_path / Path(file)\n",
    "                open(file_path, mode=\"w\").close()\n",
    "        \n",
    "    def generate_next_id(self):\n",
    "        h = hex(self.current_id)[2:]\n",
    "        while len(h) < 8:\n",
    "            h = \"0\" + h\n",
    "        self.current_id += 1\n",
    "        return h\n",
    "    \n",
    "    def import_images(self,\n",
    "                      export_dir: str,\n",
    "                      dataset_name: str,\n",
    "                      thermal: bool,\n",
    "                      get_label_function=None,\n",
    "                      get_bboxes_function=None,\n",
    "                      get_mask_function=None):\n",
    "        \n",
    "        self.thermal = thermal\n",
    "        self.mask = bool(get_mask_function or None)\n",
    "        self.bboxes = bool(get_bboxes_function or None)\n",
    "        self.label = bool(get_label_function or None)\n",
    "        self.dataset_name = dataset_name\n",
    "        \n",
    "        label = None\n",
    "        for image_name in tqdm(os.listdir(export_dir)):\n",
    "            resized = False\n",
    "            image_path = Path(export_dir) / Path(image_name)\n",
    "            self.hex_id, self.exp_path = self.generate_next_id(), image_path.as_posix()\n",
    "            \n",
    "            if get_label_function:\n",
    "                label = get_label_function(image_path)\n",
    "                with open(self.root_path / Path(\"labels.csv\"), \"a\") as labels:\n",
    "                    labels.write(f\"{self.hex_id};{str(label)};\\n\")\n",
    "            if get_bboxes_function:\n",
    "                get_bboxes_ = get_bboxes_function(self.exp_path)\n",
    "                if get_bboxes_:\n",
    "                    labels_ = [[label or \"1\"]] * len(get_bboxes_)\n",
    "                    image, bboxes = Chitra(self.exp_path,\n",
    "                                           get_bboxes_,\n",
    "                                           labels_).resize_image_with_bbox(self.ext)\n",
    "                    resized = True\n",
    "                    with open(self.root_path / Path(\"bboxes\") / Path(\".\".join([self.hex_id, \"csv\"])), \"a\") as bb_csv:\n",
    "                        for bbox in bboxes:\n",
    "                            bb_csv.write(\"\\t\".join([label[0] or str(0) if type(label) is list else label or str(0)] +  [str(int(i_)) for i_ in bbox.coords.flatten()]))\n",
    "                            bb_csv.write(\"\\n\")\n",
    "\n",
    "            if get_mask_function:\n",
    "                mask = get_mask_function(self.exp_path)\n",
    "                mask_arr = np.asarray(Image.open(mask))\n",
    "                mask_arr[mask_arr > 0] = 255\n",
    "                mask = Image.fromarray(mask_arr).resize(self.ext)\n",
    "                mask.save(self.root_path / Path(\"masks\") / Path(\".\".join([self.hex_id, \"jpg\"])))\n",
    "            \n",
    "            if not resized:\n",
    "                image = Chitra(self.exp_path).resize(self.ext)\n",
    "            image = image.convert('RGB')\n",
    "            image.save(self.root_path / Path(\"images\") / Path(\".\".join([self.hex_id, \"jpg\"])))\n",
    "\n",
    "            self.idx[self.hex_id] = {k:v for k,v in zip(self.idx_keys, (self.dataset_name,\n",
    "                                                                        self.label,\n",
    "                                                                        self.bboxes,\n",
    "                                                                        self.mask,\n",
    "                                                                        self.thermal))}\n",
    "            \n",
    "# with open(f'./{ROOT}/imgdts.idx.part_4.pkl', 'rb') as f:\n",
    "#     idx_backup = pickle.load(f)\n",
    "            \n",
    "# imgdts = ImageDataset(ROOT, restart=False, idx_b=idx_backup)\n",
    "imgdts = ImageDataset(ROOT)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a83dacbb",
   "metadata": {},
   "source": [
    "## 1. Aerial Imagery dataset for fire detection: classification and segmentation using Unmanned Aerial Vehicle (UAV)\n",
    "\n",
    "В данном датасете содержатся фото и видеозаписи.<br>\n",
    "Фото получены разложением видеозаписей на кадры.<br>\n",
    "Не все видеозаписи переведены в кадры.<br>\n",
    "Задачи: сегментация и классификация. <br>\n",
    "Данные получены с камер: оптических и тепловизионных.<br>\n",
    "Все изображения будут сводиться к размеру 640 x 640"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 311,
   "id": "8167f5a9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1-Zenmuse_X4S_1.mp4\n",
      "2-Zenmuse_X4S_2.mp4\n",
      "3-WhiteHot.mov\n",
      "4-GreenHot.mov\n",
      "5-Thermal_Fusion.mov\n",
      "6-phantom.mov\n"
     ]
    }
   ],
   "source": [
    "print(*os.listdir(\"alireza_shamsoshoara/video/\"), sep=\"\\n\") # Всего в датасете 6 видео"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 312,
   "id": "9535bd37",
   "metadata": {},
   "outputs": [],
   "source": [
    "DS_NAME = \"alireza_shamsoshoara\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "67dd7d67",
   "metadata": {},
   "source": [
    "1. Сегментационные данные взяты из `6-phantom.mov`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b5343fef",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "EXP_DIR = 'alireza_shamsoshoara/frames/Segmentation/Data/Images'\n",
    "THERM = False\n",
    "get_label_function = lambda x: \"1\"\n",
    "get_mask_function = lambda img_path: Path(img_path).parent.parent / Path(\"Masks\") / \".\".join(Path(img_path).name.split(\".\")[:-1] + [\"png\"])\n",
    "\n",
    "imgdts.import_images(export_dir=EXP_DIR,\n",
    "                     dataset_name=DS_NAME,\n",
    "                     thermal=THERM,\n",
    "                     get_label_function=get_label_function,\n",
    "                     get_mask_function=get_mask_function)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "df4f5289",
   "metadata": {},
   "source": [
    "2. Классификационные данные: \n",
    "* Тренировочные данные, лейбл \"Fire\": взяты из видео `1-Zenmuse_X4S_1.mp4`\n",
    "* Тренировочные данные, лейбл \"No_Fire\": взяты из `неизвестного источника`\n",
    "* Тестовые данные взяты из видео `6-phantom.mov`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dd3b4092",
   "metadata": {},
   "outputs": [],
   "source": [
    "EXP_DIR = 'alireza_shamsoshoara/frames/Training/Fire/'\n",
    "THERM = False\n",
    "get_label_function = lambda x: \"1\"\n",
    "\n",
    "imgdts.import_images(export_dir=EXP_DIR,\n",
    "                     dataset_name=DS_NAME,\n",
    "                     thermal=THERM,\n",
    "                     get_label_function=get_label_function)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2bb1bada",
   "metadata": {},
   "outputs": [],
   "source": [
    "EXP_DIR = 'alireza_shamsoshoara/frames/Training/No_Fire/'\n",
    "THERM = False\n",
    "get_label_function = lambda x: \"0\"\n",
    "\n",
    "imgdts.import_images(export_dir=EXP_DIR,\n",
    "                     dataset_name=DS_NAME,\n",
    "                     thermal=THERM,\n",
    "                     get_label_function=get_label_function)\n",
    "\n",
    "EXP_DIR = 'alireza_shamsoshoara/frames/Test/Fire'\n",
    "THERM = False\n",
    "get_label_function = lambda x: \"1\"\n",
    "\n",
    "imgdts.import_images(export_dir=EXP_DIR,\n",
    "                     dataset_name=DS_NAME,\n",
    "                     thermal=THERM,\n",
    "                     get_label_function=get_label_function)\n",
    "\n",
    "EXP_DIR = 'alireza_shamsoshoara/frames/Test/No_Fire'\n",
    "THERM = False\n",
    "get_label_function = lambda x: \"0\"\n",
    "\n",
    "imgdts.import_images(export_dir=EXP_DIR,\n",
    "                     dataset_name=DS_NAME,\n",
    "                     thermal=THERM,\n",
    "                     get_label_function=get_label_function)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eca963b3",
   "metadata": {},
   "source": [
    "3. В будущем, можно будет также получить данные из оставшихся видео:\n",
    "* 2-Zenmuse_X4S_2.mp4\n",
    "* 3-WhiteHot.mov\n",
    "* 4-GreenHot.mov\n",
    "* 5-Thermal_Fusion.mov"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "30d1ab28",
   "metadata": {},
   "source": [
    "## 2. UAV Fire Detection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 316,
   "id": "2f721ddc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Preparing dataset for import\n",
    "DS_NAME = \"andre3racks\"\n",
    "ann_list = os.listdir(DS_NAME + \"/annotations/\")\n",
    "\n",
    "\n",
    "def f_bndbox(obj):\n",
    "        ann_ob_l = obj if type(obj) == list else [obj]\n",
    "        return [{x[0]:x[1] if x[0] != 'bndbox' else {x_[0]:int(x_[1]) for x_ in x[1].items()} for x in o.items()} for o in ann_ob_l]\n",
    "\n",
    "\n",
    "total_ext = {}\n",
    "for ann_list_index, ann_list_element in enumerate(ann_list):\n",
    "    ann_path = DS_NAME + \"/annotations/\" + ann_list_element\n",
    "    with open(ann_path, \"r\") as xml_file:\n",
    "        xml_text = xml_file.read()\n",
    "    ann = xmltodict.parse(xml_text)[\"annotation\"]\n",
    "\n",
    "\n",
    "    ectracted = {}\n",
    "    for key in ann.keys():\n",
    "        if key == 'object':\n",
    "            ectracted[key] = f_bndbox(ann[key])\n",
    "        elif key == 'size':\n",
    "            ectracted[key] = tuple(int(ann[\"size\"][a]) for a in ann[\"size\"])\n",
    "        else:\n",
    "            ectracted[key] = ann[key]\n",
    "    image_path = \"/\".join([ectracted['folder'], ectracted['filename']])\n",
    "    ectracted[\"image_path\"] = image_path\n",
    "    total_ext[ectracted['filename']] = ectracted"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 317,
   "id": "825d37b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creating temp dir\n",
    "Path.mkdir(Path('tmp/'), exist_ok=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 318,
   "id": "95e14a12",
   "metadata": {},
   "outputs": [],
   "source": [
    "for image_path, data in total_ext.items():\n",
    "    shutil.copy('./andre3racks/' + data[\"image_path\"], 'tmp/')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f5681ac6",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Exporting dataset\n",
    "def prepare_bboxes(object_list):\n",
    "    bboxes = []\n",
    "    if type(object_list) is list:\n",
    "        for bbox in object_list:\n",
    "            xmin, ymin, xmax, ymax = bbox['bndbox']['xmin'],\\\n",
    "                                     bbox['bndbox']['ymin'],\\\n",
    "                                     bbox['bndbox']['xmax'],\\\n",
    "                                     bbox['bndbox']['ymax']\n",
    "                        \n",
    "            bboxes.append([xmin, ymin, xmax, ymax])\n",
    "    else:\n",
    "        xmin, ymin, xmax, ymax = object_list['bndbox']['xmin'],\\\n",
    "                                     object_list['bndbox']['ymin'],\\\n",
    "                                     object_list['bndbox']['xmax'],\\\n",
    "                                     object_list['bndbox']['ymax']\n",
    "                        \n",
    "        bboxes.append([xmin, ymin, xmax, ymax])\n",
    "#  \n",
    "    if len(bboxes) == 4:\n",
    "        bboxes.append([0]*4)\n",
    "    return bboxes\n",
    "\n",
    "\n",
    "EXP_DIR = 'tmp'\n",
    "THERM = False\n",
    "get_label_function = lambda ext_path_: [\"1\"] if \"object\" in total_ext[Path(ext_path_).name].keys() else [\"0\"]\n",
    "get_bboxes_function = lambda ext_path_: prepare_bboxes(total_ext[Path(ext_path_).name][\"object\"]) if \"object\" in total_ext[Path(ext_path_).name].keys() else None\n",
    "\n",
    "imgdts.import_images(export_dir=EXP_DIR,\n",
    "                     dataset_name=DS_NAME,\n",
    "                     thermal=THERM,\n",
    "                     get_label_function=get_label_function,\n",
    "                     get_bboxes_function=get_bboxes_function)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 320,
   "id": "80a9c6ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Delete tmp dir\n",
    "[os.remove('tmp/' + f) for f in os.listdir('tmp')]\n",
    "os.rmdir('tmp')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bbce4d56",
   "metadata": {},
   "source": [
    "## 3. Forest Fire Detection through UAV imagery using CNNs"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5e8f94ae",
   "metadata": {},
   "source": [
    "Простой датасет для классификации. Тепловизионных снимков нет, датасет разделён на папки."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 321,
   "id": "565367b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "DS_NAME = \"leading_india_ai\"\n",
    "THERM = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4f66321e",
   "metadata": {},
   "outputs": [],
   "source": [
    "EXP_DIR = \"leading_india_ai/data/train/Fire/\"\n",
    "get_label_function = lambda x: \"1\"\n",
    "imgdts.import_images(export_dir=EXP_DIR,\n",
    "                     dataset_name=DS_NAME,\n",
    "                     thermal=THERM,\n",
    "                     get_label_function=get_label_function)\n",
    "\n",
    "EXP_DIR = \"leading_india_ai/data/train/No Fire/\"\n",
    "get_label_function = lambda x: \"0\"\n",
    "imgdts.import_images(export_dir=EXP_DIR,\n",
    "                     dataset_name=DS_NAME,\n",
    "                     thermal=THERM,\n",
    "                     get_label_function=get_label_function)\n",
    "\n",
    "EXP_DIR = \"leading_india_ai/data/validation/Fire/\"\n",
    "get_label_function = lambda x: \"1\"\n",
    "imgdts.import_images(export_dir=EXP_DIR,\n",
    "                     dataset_name=DS_NAME,\n",
    "                     thermal=THERM,\n",
    "                     get_label_function=get_label_function)\n",
    "\n",
    "EXP_DIR = \"leading_india_ai/data/validation/No Fire/\"\n",
    "get_label_function = lambda x: \"0\"\n",
    "imgdts.import_images(export_dir=EXP_DIR,\n",
    "                     dataset_name=DS_NAME,\n",
    "                     thermal=THERM,\n",
    "                     get_label_function=get_label_function)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "420e9857",
   "metadata": {},
   "source": [
    "## 4. UAV Thermal Imaginary - Fire Dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7e0de98f",
   "metadata": {},
   "source": [
    "Тепловизионный датасет"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 323,
   "id": "b459bc34",
   "metadata": {},
   "outputs": [],
   "source": [
    "DS_NAME = \"uav_thermal_imaginary_fire_dataset\"\n",
    "THERM = True\n",
    "\n",
    "split_list = os.listdir(DS_NAME)\n",
    "class_list = np.unique([os.listdir(\"/\".join([DS_NAME, split])) for split in split_list]).tolist()\n",
    "\n",
    "# Creating temp dir\n",
    "Path.mkdir(Path('tmp'), exist_ok=True)\n",
    "\n",
    "# Function for binarise thermal image\n",
    "def from_color_to_binary(im_p_):\n",
    "    im_ = np.asarray(Image.open(im_p_))\n",
    "    gray_ = cv2.cvtColor(im_, cv2.COLOR_RGB2GRAY)\n",
    "    (_, bw_) = cv2.threshold(gray_, 127, 255, cv2.THRESH_BINARY)\n",
    "    Image.fromarray(bw_)\n",
    "    return bw_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9e4e4e03",
   "metadata": {},
   "outputs": [],
   "source": [
    "for split_ in split_list:\n",
    "    for class_ in class_list:\n",
    "        folder_ = '/'.join([DS_NAME, split_, class_])\n",
    "        Path.mkdir(Path('/'.join([\"tmp\", split_, class_])), exist_ok=True, parents=True)\n",
    "        for file_ in os.listdir(folder_):\n",
    "            bw_ = from_color_to_binary('/'.join([folder_, file_]))\n",
    "            Image.fromarray(bw_).save('/'.join([\"tmp\", split_, class_, file_]))\n",
    "        EXP_DIR = \"/\".join([\"tmp\", split_, class_])\n",
    "        get_label_function = lambda x: \"0\" if class_ == 'no_fire' else \"1\" \n",
    "        imgdts.import_images(export_dir=EXP_DIR,\n",
    "                     dataset_name=DS_NAME,\n",
    "                     thermal=THERM,\n",
    "                     get_label_function=get_label_function)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 325,
   "id": "df3c564f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Delete tmp dir\n",
    "shutil.rmtree('tmp')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "98bfd2ff",
   "metadata": {},
   "source": [
    "## 5. Создание индекс-файла"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eeac0b0e",
   "metadata": {},
   "source": [
    "Сначала нужно собрать воедино всю информацию о файлах"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "6e4123b3",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████████████████████████████████████████████████████████████████████████████████████████| 57105/57105 [01:43<00:00, 551.82it/s]\n"
     ]
    }
   ],
   "source": [
    "index = imgdts.idx\n",
    "\n",
    "df = pd.DataFrame(pd.read_csv(F\"./{ROOT}/labels.csv\", sep=\";\").values[:,:2], columns=[\"hex\", \"label\"])\n",
    "df[\"label\"] = df[\"label\"].apply(lambda x: int(x) if len(x) == 1 else int(x[2]))\n",
    "df.drop_duplicates(inplace=True)\n",
    "\n",
    "t_ls = []\n",
    "for key, values in sorted(index.items()):\n",
    "    a = {\"hex\":key}\n",
    "    if len(values) > 0:\n",
    "        t_ls.append(a | values)\n",
    "    else:\n",
    "        print(\"err\")\n",
    "\n",
    "\n",
    "tot_df = pd.DataFrame(columns=df.columns)\n",
    "for t in tqdm(t_ls):\n",
    "    tot_df = pd.concat([tot_df, pd.DataFrame.from_records([t])])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "4f43ad2e",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "57104it [03:06, 306.95it/s]\n"
     ]
    }
   ],
   "source": [
    "for hex_id, val_ in tqdm(df.set_index(\"hex\").iterrows()):\n",
    "    val_ = val_.values[0]\n",
    "    tot_df.loc[tot_df[\"hex\"] == hex_id, \"cls\"] = val_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "3c8367ef",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "проверить данные для локализации\n"
     ]
    }
   ],
   "source": [
    "# Dataframe check\n",
    "if tot_df[\"bboxes\"].sum() != len(os.listdir(f\"{ROOT}/bboxes/\")):\n",
    "    print(\"проверить данные для локализации\")\n",
    "if tot_df.shape[0] != len(os.listdir(f\"{ROOT}/images/\")):\n",
    "    print(\"проверить изображения\")\n",
    "if tot_df[\"mask\"].sum() != len(os.listdir(f\"{ROOT}/masks/\")):\n",
    "    print(\"проверить данные для сегментации\")\n",
    "if tot_df[\"label\"].sum() != pd.read_csv(f\"{ROOT}/labels.csv\", delimiter=\";\", header=None).shape[0]:\n",
    "    print(\"проверить данные для сегментации\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "2f0a7f80",
   "metadata": {},
   "outputs": [],
   "source": [
    "def masks_count():\n",
    "    src_path = \"./andre3racks/annotations/\"\n",
    "    filenames = os.listdir(src_path)\n",
    "    anns = [\"\".join([a,b]) for a,b in zip([src_path] * len(filenames), [*filenames])]\n",
    "\n",
    "    def read_path(path):\n",
    "        with open(path, \"r\") as f:\n",
    "            text = f.read()\n",
    "        return text\n",
    "\n",
    "    texts = [read_path(an) for an in anns]\n",
    "\n",
    "    return sum(1 if \"bndbox\" in t else 0 for t in texts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "1edf6cd4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "In dataframe 1034; in folder 725; real 725\n"
     ]
    }
   ],
   "source": [
    "print(\"In dataframe {}; in folder {}; real {}\".format(tot_df[\"bboxes\"].sum(),\n",
    "                                                           len(os.listdir(f\"{ROOT}/bboxes/\")),\n",
    "                                                           masks_count()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "72564583",
   "metadata": {},
   "outputs": [],
   "source": [
    "tot_df.set_index(\"hex\", inplace=True)\n",
    "real_bboxes = [i.split(\".\")[0] for i in os.listdir(f\"{ROOT}/bboxes/\")]\n",
    "bad_inxs = tot_df[~tot_df.index.isin(real_bboxes) & tot_df.bboxes].index\n",
    "tot_df.loc[bad_inxs, \"bboxes\"] = tot_df.loc[bad_inxs, \"bboxes\"].replace(True, False)\n",
    "\n",
    "assert tot_df[\"bboxes\"].sum() == len(os.listdir(f\"{ROOT}/bboxes/\")), (\"проверить данные для локализации\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "e27c35f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "tot_df = tot_df[[\"bboxes\", \"mask\", \"cls\"]]\n",
    "tot_df.loc[tot_df.index, \"thermal\"] = False\n",
    "thermal_index = [i for i, v in index.items() if v[\"thermal\"]]\n",
    "tot_df.loc[thermal_index, \"thermal\"] = True\n",
    "tot_df.loc[tot_df.index, \"thermal\"] = False\n",
    "thermal_index = [i for i, v in index.items() if v[\"thermal\"]]\n",
    "tot_df.loc[thermal_index, \"thermal\"] = True\n",
    "d = tot_df\n",
    "d.to_csv(\"index.csv\", index=\"hex\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f97e647b",
   "metadata": {},
   "source": [
    "# Интерфейс создания датасета:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 162,
   "id": "fceec9ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "def copy_resourses(id_, task_, split_, labels_index):\n",
    "    \n",
    "    src_img = f'{ROOT}/images/{id_}.jpg'\n",
    "\n",
    "    if task_ == \"cls\":\n",
    "\n",
    "        label = \"fire\" if int(labels_index.iloc[int(id_, 16),0]) == 1 else \"no_fire\"\n",
    "        dst_img = f'{PARAMS[\"output_dir\"]}/{split_}/images/{label}/{id_}.jpg'\n",
    "        Path.mkdir(Path(dst_img).parent, parents=True, exist_ok=True)\n",
    "        \n",
    "\n",
    "    if task_ == \"loc\":\n",
    "        dst_img = f'{PARAMS[\"output_dir\"]}/{split_}/images/{id_}.jpg'\n",
    "        \n",
    "        src_bb = f'{ROOT}/bboxes/{id_}.jpg'\n",
    "        dst_bb = f\"{PARAMS['output_dir']}/{split_}/images/{id_}.txt\"\n",
    "        Path.mkdir(Path(dst_img).parent, parents=True, exist_ok=True)\n",
    "        shutil.copyfile(src_bb, dst_bb)\n",
    "\n",
    "    if task_ == \"seg\":\n",
    "        dst_img = f'{PARAMS[\"output_dir\"]}/{split_}/images/{id_}.jpg'\n",
    "        \n",
    "        src_mask = f'{ROOT}/masks/{id_}.jpg'\n",
    "        dst_mask = f\"{PARAMS['output_dir']}/{split_}/masks/{id_}.jpg\"\n",
    "        Path.mkdir(Path(dst_img).parent, parents=True, exist_ok=True)\n",
    "        shutil.copyfile(src_mask, dst_mask)\n",
    "        \n",
    "    Path.mkdir(Path(dst_img).parent, parents=True, exist_ok=True)\n",
    "    shutil.copyfile(src_img, dst_img)\n",
    "\n",
    "def split_data(index_list, split_ration):\n",
    "    \n",
    "    assert 0.99 < sum((0.7, 0.2, 0.1)) < 1.01, (f\"Неверные доли разбивки {split_ration}\")\n",
    "    \n",
    "    n = len(index_list)\n",
    "    tr_n = int(split_ration[0] * n)\n",
    "    te_n = int(split_ration[1] * n)\n",
    "    va_n = n - te_n - tr_n\n",
    "\n",
    "    tr_l, te_l, va_l = [], [], []\n",
    "\n",
    "    for _ in range(tr_n):\n",
    "        ch_ = random.choice(index_list)\n",
    "        tr_l.append(ch_)\n",
    "        index_list.remove(ch_)\n",
    "    \n",
    "    for _ in range(te_n):\n",
    "        ch_ = random.choice(index_list)\n",
    "        te_l.append(ch_)\n",
    "        index_list.remove(ch_)\n",
    "\n",
    "    va_l = index_list\n",
    "    \n",
    "    return {key:val for key, val in zip((\"train\", \"test\", \"val\"), (tr_l, te_l, va_l))} \n",
    "\n",
    "# task_filter\n",
    "def task_filter(task, labels_index):\n",
    "    assert task in [\"cls\", \"loc\", \"seg\"], (\"Wrong task param!\")\n",
    "    if task == \"cls\":\n",
    "        return True\n",
    "    if task == \"loc\":\n",
    "        return d.bboxes\n",
    "    return d[\"mask\"]\n",
    "\n",
    "\n",
    "cam_filter = lambda c_: d.thermal if c_ == \"thermal\" else ~d.thermal\n",
    "\n",
    "def gen_dataset(task:str,\n",
    "                cam:str = \"optic\",\n",
    "                split_ration:tuple = (0.8, 0.15, 0.5),\n",
    "                output_dir:str = \"./default/\",\n",
    "               ):\n",
    "    \"\"\"\n",
    "    Generates dataset with suitable parameters\n",
    "    \n",
    "    :param task: \"cls\" for classification,\n",
    "                 \"loc\" for localisation,\n",
    "                 \"seg\" for segmentation;\n",
    "    :param cam:  \"therm\" for thermal,\n",
    "                 \"optic\" for optic cam;\n",
    "    :param split_ration: tuple of three floats: train_ratio, test_ratio, val_ratio\n",
    "                         sum(split_ration) must be 1\n",
    "    :param output_dir:   relative path for dataset creation\n",
    "    \n",
    "    \"\"\"\n",
    "    \n",
    "    labels_index = pd.read_csv(f\"{ROOT}/labels.csv\", sep=\";\", header=None, index_col=0)\n",
    "    \n",
    "    # Labels fix\n",
    "    t = labels_index.iloc[:,:2]\n",
    "    t_v = labels_index[1].values\n",
    "    t_v = [int(i) if len(i) == 1 else int(i[2]) for i in t_v]\n",
    "    labels_index[1] = t_v\n",
    "    \n",
    "    # Filtering images\n",
    "    df_filter = task_filter(task, labels_index) & cam_filter(cam)\n",
    "    index_list = d[df_filter].index\n",
    "    \n",
    "    if len(index_list):\n",
    "        print(f\"Found {len(index_list)} images for current parameters.\\nCreating dataset\")\n",
    "    else:\n",
    "        print(\"No images found for current parameters\")\n",
    "        \n",
    "    \n",
    "    \n",
    "    # Copying data\n",
    "    for split, i_val in split_data(list(index_list), split_ration).items():\n",
    "        for hex_id in i_val:\n",
    "            copy_resourses(hex_id, PARAMS[\"task\"], split, labels_index)\n",
    "            \n",
    "    print(\"Dataset created!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 307,
   "id": "53b9ec8b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 53125 images for current parameters.\n",
      "Creating dataset\n",
      "Dataset created!\n"
     ]
    }
   ],
   "source": [
    "d = pd.read_csv(\"index.csv\", index_col=\"hex\")\n",
    "gen_dataset(\"cls\", \"optic\", (0.7, 0.2, 0.1), \"cls_test\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9c6c9827",
   "metadata": {},
   "source": [
    "# Скрипт для создания датасета\n",
    "\n",
    "Окончательные правки были добавлены в скрипт"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9741b783",
   "metadata": {},
   "source": [
    "Использование (запускать из папки проекта):\n",
    "\n",
    "`python create_dataset.py \"task\" \"cam\" \"split_ration\" \"output_dir\"`\n",
    "\n",
    "* task - тип задачи для которой нужно сделать датасет (\"cls\" for classification, \"loc\" for localisation, \"seg\" for segmentation)\n",
    "* cam - тип камеры с котрой делали сники (\"therm\" for thermal, \"optic\" for optic cam)\n",
    "* split_ration - доли тренировочной, тестовой и валидационной частей через пробел\n",
    "`\"0.75 0.1 0.05\"`\n",
    "* output_dir - относительный путь папки, в которой требуется создать датасет\n",
    "\n",
    "Например:\n",
    "\n",
    "`python create_dataset.py \"loc\" \"optic\" \"0.7 0.2 0.1\" \"final_test\"`"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
